import pandas as pd
import numpy as np
import pickle
import torch

from sklearn.model_selection import train_test_split
from tqdm import tqdm
import datetime

#from Tabular.xailib.explainers.lore_explainer import LoreTabularExplainer
from MyLoreSA.lorem_new import LOREU
from MyLoreSA.util import get_tuned_classifier, load_datasets, transform_datasets, nonrejected_accuracy, rejection_classification_report, get_rejected_list, classification_quality, rejection_quality
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from best_params_clf import best_clf_dict, best_params_dict, source_file_dict, class_field_dict
from cf_parameters import neigh_kwargs
import argparse
from MyLoreSA.l2r_lore import L2R_LORE
from Lib.L2R.code.model_agnostic import PlugInRule, PlugInRuleAUC, SCRoss
 
parser = argparse.ArgumentParser(description="",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-d", "--datasets", nargs='+', type=str, default='german_credit')
parser.add_argument("-test_size", "--test_size", type=int, default=-1)
parser.add_argument("-u", "--uncertainty_threshold", type=float, default=0.5)
args = parser.parse_args()
config = vars(args)

datasets = config['datasets']

if torch.cuda.is_available():
    device = 'cuda'
    tree_method_xgb = 'gpu_hist'
else:
    device = 'cpu'
    tree_method_xgb = 'auto'

# Load and transform dataset
df_dict = load_datasets(source_file_dict)
df_dict_new = transform_datasets(df_dict, class_field_dict)

clf_dict = {
    #'RandomForest' : RandomForestClassifier(random_state=123),
    'MLPClassifier' : MLPClassifier(random_state=123, early_stopping=True),
    #'TabNet' : TabNetClassifier(device_name=device, verbose=0, seed=123),
    'XGBoost' : XGBClassifier(tree_method=tree_method_xgb, random_state=123),
    'SVC' : SVC(probability=True, random_state=123)
}


neigh_type = 'rndgenp' # the generation you want (random, genetic, geneticp, cfs, rndgen)
binary = 'binary_from_dts' #how to merge the trees (binary from dts, binary from bb are creating a binary tree, nari is creating a n ari tree)
n = 1000 # neighborhood size (i.e., 2n instances in total) --> in realt√† dovrebbe essere sample
n_neigh = 150
cxpb = 0.7 # values to set for the genetic generation
mutpb = 0.5 #values to set for the genetic generation
unc_thr = config['uncertainty_threshold'] # TO DO: finetuning
uncertainty_metric = 'max' # max or ratio
test_size = config['test_size'] # TO DO: add option None to train lore on train-test split
extract_counterfactuals_by='min_distance'
optimize_tau_by='nonrejected_accuracy' 
counterfactual_metric  = 'mixed'


df_dict_new_small = {k:v for k,v in df_dict_new.items() if k in datasets}
print('The following datasets will be analyzed: ', df_dict_new_small.keys())

all_d_results = dict()
for d, d_val in df_dict_new_small.items():
    
    # for each dataset, optimize the rejection policy and compute predictions/rejections with explanations
    X = d_val[0].drop(columns=class_field_dict[d])
    y = d_val[0][class_field_dict[d]]

    if test_size == -1:
        test_size == X.shape[0]
    
    numeric_columns, categorical_columns = [], []
    for i, col in enumerate(X.columns):
        if col in d_val[3]:
            numeric_columns.append(i)
        else:
            categorical_columns.append(i)
    
    #X, y = X.head(100), y.head(100)

    # here we create test set using perc_test and calibration set using perc_hold
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        test_size=0.2,
                                                        random_state=1234,
                                                        stratify=y)
    X_train, X_cal, y_train, y_cal = train_test_split(X_train, y_train,
                                                        test_size=0.125,
                                                        random_state=1234,
                                                        stratify=y_train)
    d_result = dict()

    # Generate counterfactuals for the best classifier only
    clf = best_clf_dict[d]
    print('Dataset: ', d, ' - Classifier: ', clf, ' - Training size: ', X_train.shape[0])
    bb = get_tuned_classifier(d, clf, clf_dict, best_params_dict)


    # here we define the selective classifiers
    # plugin : plug-in rule by Herbei and Wegkamp
    # plugin_auc : plug-in rule by Pugnana and Ruggieri, 2023b - a plug-in rule that improves AUC when abstaining
    # scr : cross-fitting approach by Pugnana and Ruggieri, 2023a - a method that uses cross-fitting to avoid overfitting and reserve some data for calibration
    plg = PlugInRule(bb)
    plg_auc = PlugInRuleAUC(bb)
    scr = SCRoss(bb)
    
    print('Fitting data')   
    # here we train the selective classifiers
    plg.fit(X_train.to_numpy(), y_train.to_numpy())
    plg_auc.fit(X_train.to_numpy(), y_train.to_numpy())
    scr.fit(X_train.to_numpy(), y_train.to_numpy())


    # calibrate selective classifiers for target coverages
    rej_rate_list = np.linspace(0, 0.99, num=100)
    target_coverages = [1-n for n in rej_rate_list]

    # target_coverages = [.99, .75, .50] # here we define the target coverages for the selective classifiers
    plg.calibrate(X_cal, target_coverages=target_coverages)
    plg_auc.calibrate(X_cal, y_cal, target_coverages=target_coverages)
    scr.calibrate(target_coverages=target_coverages)


    print('Computing predictions')
    # // Evaluate selective classifiers
    # here we create an array containing levels for acceptance depending on how the selective classifiers are calibrated
    # since we used 100 target coverages, we have 101 levels of acceptance, i.e., 0, 1, 2, 3, ..., 100
    selected_plg = plg.qband(X_test)
    selected_plg_auc = plg_auc.qband(X_test)
    selected_scr = scr.qband(X_test)



    print('Evaluating the selective classifier')
    #y_pred_plg = plg.predict(df_test[atts])
    rejected_by_coverage_plg = dict()
    rejected_by_coverage_plgauc = dict()
    rejected_by_coverage_scr = dict()
    n = len(y_test)

    for i in range(100):
        qband = i
        rej_rate = 1 - target_coverages[i] # save as rejection rate, not coverage
        #rejected list is a 0/1 list with 1 at rejected indices
        rejected_list_plg = get_rejected_list(selected_plg, i)
        rejected_list_plgauc = get_rejected_list(selected_plg_auc, i)
        rejected_list_scr = get_rejected_list(selected_scr, i)
    
        correct_nonrejected_plg, correct_rejected_plg, miscl_nonrejected_plg, miscl_rejected_plg, df_plg = rejection_classification_report(y_test, 
                                                                                                               plg.predict(X_test), 
                                                                                                               rejected_list_plg)
    
        correct_nonrejected_plgauc, correct_rejected_plgauc, miscl_nonrejected_plgauc, miscl_rejected_plgauc, df_plgauc = rejection_classification_report(y_test, 
                                                                                                               plg_auc.predict(X_test), 
                                                                                                               rejected_list_plgauc)
    
        correct_nonrejected_scr, correct_rejected_scr, miscl_nonrejected_scr, miscl_rejected_scr, df_scr = rejection_classification_report(y_test, 
                                                                                                               scr.predict(X_test), 
                                                                                                               rejected_list_scr)

        # get performance metrics
        AN_plg = nonrejected_accuracy(correct_nonrejected_plg, miscl_nonrejected_plg)
        AN_plgauc = nonrejected_accuracy(correct_nonrejected_plgauc, miscl_nonrejected_plgauc)
        AN_scr = nonrejected_accuracy(correct_nonrejected_scr, miscl_nonrejected_scr)
        
        CQ_plg = classification_quality(correct_nonrejected_plg, miscl_rejected_plg, n)
        CQ_plgauc = classification_quality(correct_nonrejected_plgauc, miscl_rejected_plgauc, n)
        CQ_scr = classification_quality(correct_nonrejected_scr, miscl_rejected_scr, n)
        
        
        RQ_plg = rejection_quality(correct_rejected_plg, correct_nonrejected_plg, miscl_rejected_plg, miscl_nonrejected_plg)
        RQ_plgauc = rejection_quality(correct_rejected_plgauc, correct_nonrejected_plgauc, miscl_rejected_plgauc, miscl_nonrejected_plgauc)
        RQ_scr = rejection_quality(correct_rejected_scr, correct_nonrejected_scr, miscl_rejected_scr, miscl_nonrejected_scr)
    
    
        rejected_by_coverage_plg[rej_rate] = {
            'classification_quality': CQ_plg,
            'rejection_quality'     : RQ_plg,
            'nonrejected_accuracy'  : AN_plg
            }
        rejected_by_coverage_plgauc[rej_rate] = {
            'classification_quality': CQ_plgauc,
            'rejection_quality'     : RQ_plgauc,
            'nonrejected_accuracy'  : AN_plgauc
            }
        rejected_by_coverage_scr[rej_rate] = {
            'classification_quality': CQ_scr,
            'rejection_quality'     : RQ_scr,
            'nonrejected_accuracy'  : AN_scr
            }






    # prediction_results = lore.predict(X_test)
    # y_preds = [y.prediction for y in prediction_results['y_predictions']]
    # n_rejected_best = len(prediction_results['rejected_list'])
    
    # print('Evaluating the selective classifier')
    # evaluation_results = lore.evaluate_l2r(len(prediction_results['rejected_list']), 
    #                                        prediction_results['distance_dict'], 
    #                                        y_test, y_preds)

    results = {'PLG': rejected_by_coverage_plg,
               'PLGAUC' : rejected_by_coverage_plgauc,
               'SCR': rejected_by_coverage_scr
               }

    ct = datetime.datetime.now()
    ct = ct.strftime("%m_%d_%Y_%H_%M")
    with open('Results/baseline_results_'+ct+'.pickle', "wb") as pickle_file:
        pickle.dump(results, pickle_file)
