import pandas as pd
import numpy as np
import pickle
import torch

from sklearn.model_selection import train_test_split
from tqdm import tqdm
import datetime

#from Tabular.xailib.explainers.lore_explainer import LoreTabularExplainer
from MyLoreSA.lorem_new import LOREU
from MyLoreSA.util import get_tuned_classifier, load_datasets, transform_datasets, compute_rejection_policy, compute_distance_from_counterfactual
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from best_params_clf import best_clf_dict, best_params_dict, source_file_dict, class_field_dict
from cf_parameters import neigh_kwargs
import argparse
from MyLoreSA.l2r_lore import L2R_LORE
from Lib.L2R.code.model_agnostic import PlugInRule, PlugInRuleAUC, SCRoss
 
parser = argparse.ArgumentParser(description="",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-d", "--datasets", nargs='+', type=str, default='german_credit')
parser.add_argument("-test_size", "--test_size", type=int, default=-1)
parser.add_argument("-u", "--uncertainty_threshold", type=float, default=0.5)
args = parser.parse_args()
config = vars(args)

datasets = config['datasets']

if torch.cuda.is_available():
    device = 'cuda'
    tree_method_xgb = 'gpu_hist'
else:
    device = 'cpu'
    tree_method_xgb = 'auto'

# Load and transform dataset
df_dict = load_datasets(source_file_dict)
df_dict_new = transform_datasets(df_dict, class_field_dict)

clf_dict = {
    #'RandomForest' : RandomForestClassifier(random_state=123),
    'MLPClassifier' : MLPClassifier(random_state=123, early_stopping=True),
    #'TabNet' : TabNetClassifier(device_name=device, verbose=0, seed=123),
    'XGBoost' : XGBClassifier(tree_method=tree_method_xgb, random_state=123),
    'SVC' : SVC(probability=True, random_state=123)
}


neigh_type = 'rndgenp' # the generation you want (random, genetic, geneticp, cfs, rndgen)
binary = 'binary_from_dts' #how to merge the trees (binary from dts, binary from bb are creating a binary tree, nari is creating a n ari tree)
n = 1000 # neighborhood size (i.e., 2n instances in total) --> in realt√† dovrebbe essere sample
n_neigh = 150
cxpb = 0.7 # values to set for the genetic generation
mutpb = 0.5 #values to set for the genetic generation
unc_thr = config['uncertainty_threshold'] # TO DO: finetuning
uncertainty_metric = 'max' # max or ratio
test_size = config['test_size'] # TO DO: add option None to train lore on train-test split
extract_counterfactuals_by='min_distance'
optimize_tau_by='nonrejected_accuracy' 
counterfactual_metric  = 'mixed'


df_dict_new_small = {k:v for k,v in df_dict_new.items() if k in datasets}
print('The following datasets will be analyzed: ', df_dict_new_small.keys())

all_d_results = dict()
for d, d_val in df_dict_new_small.items():
    
    # for each dataset, optimize the rejection policy and compute predictions/rejections with explanations
    X = d_val[0].drop(columns=class_field_dict[d])
    y = d_val[0][class_field_dict[d]]

    if test_size == -1:
        test_size == X.shape[0]
    
    numeric_columns, categorical_columns = [], []
    for i, col in enumerate(X.columns):
        if col in d_val[3]:
            numeric_columns.append(i)
        else:
            categorical_columns.append(i)
    
    #X, y = X.head(100), y.head(100)

    # here we create test set using perc_test and calibration set using perc_hold
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        test_size=0.2,
                                                        random_state=1234,
                                                        stratify=y)
    X_train, X_cal, y_train, y_cal = train_test_split(X_train, y_train,
                                                        test_size=0.125,
                                                        random_state=1234,
                                                        stratify=y_train)
    d_result = dict()

    # Generate counterfactuals for the best classifier only
    clf = best_clf_dict[d]
    print('Dataset: ', d, ' - Classifier: ', clf, ' - Training size: ', X_train.shape[0])
    bb = get_tuned_classifier(d, clf, clf_dict, best_params_dict)


    # here we define the selective classifiers
    # plugin : plug-in rule by Herbei and Wegkamp
    # plugin_auc : plug-in rule by Pugnana and Ruggieri, 2023b - a plug-in rule that improves AUC when abstaining
    # scr : cross-fitting approach by Pugnana and Ruggieri, 2023a - a method that uses cross-fitting to avoid overfitting and reserve some data for calibration
    plg = PlugInRule(bb)
    plg_auc = PlugInRuleAUC(bb)
    scr = SCRoss(bb)
    
    print('Fitting data')    
    # here we train the selective classifiers
    plg.fit(X_train, y_train)
    plg_auc.fit(X_train, y_train)
    scr.fit(X_train, y_train)


    #calibrate selective classifiers for target coverages
    rej_rate_list = np.linspace(0, 0.99, num=100)
    target_coverages = [1-n for n in rej_rate_list]

    #target_coverages = [.99, .75, .50] # here we define the target coverages for the selective classifiers
    plg.calibrate(X_cal, target_coverages=target_coverages)
    plg_auc.calibrate(X_cal, y_cal, target_coverages=target_coverages)
    scr.calibrate(target_coverages=target_coverages)


    print('Computing predictions')
    # // Evaluate selective classifiers
    # here we create an array containing levels for acceptance depending on how the selective classifiers are calibrated
    # since we used three target coverages, we have four levels of acceptance, i.e., 0, 1, 2, 3
    selected_plg = plg.qband(X_test)
    selected_plg_auc = plg_auc.qband(X_test)
    selected_scr = scr.qband(X_test)

    prediction_results = lore.predict(X_test)
    y_preds = [y.prediction for y in prediction_results['y_predictions']]
    n_rejected_best = len(prediction_results['rejected_list'])
    
    print('Evaluating the selective classifier')
    evaluation_results = lore.evaluate_l2r(len(prediction_results['rejected_list']), 
                                           prediction_results['distance_dict'], 
                                           y_test, y_preds)

    results = {'training': training_results, 
               'calibration': calibration_results,
               'predictions': prediction_results,
               'evaluation': evaluation_results
               }

    ct = datetime.datetime.now()
    ct = ct.strftime("%m_%d_%Y_%H_%M")
    with open('Results/l2lore_results_'+ct+'.pickle', "wb") as pickle_file:
        pickle.dump(results, pickle_file)
