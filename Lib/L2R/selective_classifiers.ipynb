{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# // Load data\n",
    "perc_hold = 0.125\n",
    "perc_test = 0.2\n",
    "# the name of columns of the dataset\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"work_hours\",\n",
    "    \"native_country\",\n",
    "    \"inc\",\n",
    "]\n",
    "\n",
    "# here we shuffle train and test set\n",
    "df_train = pd.read_csv(\"data/raw/adult.data\", names=columns)\n",
    "df_test = pd.read_csv(\"data/raw/adult.test\", names=columns, skiprows=1)\n",
    "df_train[\"TARGET\"] = np.where(df_train[\"inc\"] == \" >50K\", 1, 0)\n",
    "df_test[\"TARGET\"] = np.where(df_test[\"inc\"] == \" >50K.\", 1, 0)\n",
    "df_train.drop(columns=[\"inc\", \"education\"], inplace=True) # here we drop additional columns that we do not need\n",
    "df_test.drop(columns=[\"inc\", \"education\"], inplace=True) # here we drop additional columns that we do not need\n",
    "df = pd.concat([df_train, df_test], axis=0) #concat the two dataset\n",
    "df.reset_index(inplace=True) #reset index\n",
    "#here we define the final columns of the dataset\n",
    "atts = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"work_hours\",\n",
    "    \"native_country\",\n",
    "]\n",
    "# here we distinguish between categorical and numerical attributes\n",
    "cat_atts = []\n",
    "cont_atts = []\n",
    "for col in atts:\n",
    "    if df_train[col].dtype == object:\n",
    "        #         df_train[col] = df_train[col].astype('category').cat.codes\n",
    "        cat_atts.append(col)\n",
    "    else:\n",
    "        #         df_train[col] = df_train[col].astype(float)\n",
    "        cont_atts.append(col)\n",
    "# here we one hot encode categorical attributes\n",
    "df = pd.get_dummies(df, columns=cat_atts)\n",
    "# here we remove the extra spaces from the column names of the dataset\n",
    "df.columns = [col.replace(\" \", \"\") for col in df.columns]\n",
    "# here we remove the \"weird\" characters from the column names of the dataset\n",
    "df.columns = [col.replace(\"?\", \"UNK\") for col in df.columns]\n",
    "# here we define the final set of features to be used in the classification task\n",
    "atts = [col for col in df.columns if col not in [\"TARGET\", \"index\"]]\n",
    "# here we create test set using perc_test and calibration set using perc_hold\n",
    "df_train_, df_test = train_test_split(\n",
    "    df, random_state=42, test_size=perc_test, stratify=df[\"TARGET\"]\n",
    ")\n",
    "df_train, df_cal = train_test_split(\n",
    "    df_train_, random_state=42, test_size=perc_hold, stratify=df_train_[\"TARGET\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# The selective classification framework\n",
    "\n",
    "Selective classification allows classifiers to abstain from predicting, avoiding making mistakes over difficult instances. In this notebook we show how to use the selective classification framework to train selective classifiers and evaluate them. We use the Adult dataset, which is a binary classification task. We use the LightGBM classifier as base model for the selective classifiers. We train the selective classifiers using the training set, calibrate them using the calibration set, and evaluate them using the test set. We use the accuracy as metric for the evaluation of the selective classifiers. We also show how to use the AUC as metric for the evaluation of the selective classifiers.\n",
    "\n",
    "More formally we have:\n",
    "- $X$ are the features\n",
    "- $Y$ are the target variables\n",
    "- $f: X \\rightarrow Y$ is the base classifier\n",
    "- $g: X \\rightarrow \\{0,1\\}$ is the selection function\n",
    "\n",
    "The selective classifier is then the pair $(f,g)$, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "(f,g)(x) = \\begin{cases}\n",
    "f(x) \\quad \\text{if } g(x) = 1 \\\\\n",
    "\\text{abstain} \\quad \\text{if } g(x) = 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "To build a selective classifier, one has to specify a target coverage $c \\in [0,1]$, which is the percentage of instances that the selective classifier is allowed to abstain from predicting. The selective classifier is then calibrated to predict on $c$ fraction of the instances. Depending on the methodology, one might need a calibration set to calibrate the selective classifier to achieve the target coverage. Ideally, the more we abstain the more we reduce the number of mistakes, hence performance should improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# // Train selective classifiers\n",
    "from code.model_agnostic import PlugInRule, PlugInRuleAUC, SCRoss\n",
    "# here we define the base model to be used by the selective classifiers\n",
    "clf_base = LGBMClassifier(random_state=42)\n",
    "# here we define the selective classifiers\n",
    "# plugin : plug-in rule by Herbei and Wegkamp\n",
    "# plugin_auc : plug-in rule by Pugnana and Ruggieri, 2023b - a plug-in rule that improves AUC when abstaining\n",
    "# scr : cross-fitting approach by Pugnana and Ruggieri, 2023a - a method that uses cross-fitting to avoid overfitting and reserve some data for calibration\n",
    "plg = PlugInRule(clf_base)\n",
    "plg_auc = PlugInRuleAUC(clf_base)\n",
    "scr = SCRoss(clf_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8180, number of negative: 26008\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 750\n",
      "[LightGBM] [Info] Number of data points in the train set: 34188, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239265 -> initscore=-1.156712\n",
      "[LightGBM] [Info] Start training from score -1.156712\n",
      "[LightGBM] [Info] Number of positive: 8180, number of negative: 26008\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 750\n",
      "[LightGBM] [Info] Number of data points in the train set: 34188, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239265 -> initscore=-1.156712\n",
      "[LightGBM] [Info] Start training from score -1.156712\n",
      "[LightGBM] [Info] Number of positive: 7479, number of negative: 23779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 743\n",
      "[LightGBM] [Info] Number of data points in the train set: 31258, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239267 -> initscore=-1.156704\n",
      "[LightGBM] [Info] Start training from score -1.156704\n",
      "[LightGBM] [Info] Number of positive: 7479, number of negative: 23779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 749\n",
      "[LightGBM] [Info] Number of data points in the train set: 31258, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239267 -> initscore=-1.156704\n",
      "[LightGBM] [Info] Start training from score -1.156704\n",
      "[LightGBM] [Info] Number of positive: 7479, number of negative: 23779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002526 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 747\n",
      "[LightGBM] [Info] Number of data points in the train set: 31258, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239267 -> initscore=-1.156704\n",
      "[LightGBM] [Info] Start training from score -1.156704\n",
      "[LightGBM] [Info] Number of positive: 7480, number of negative: 23779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 750\n",
      "[LightGBM] [Info] Number of data points in the train set: 31259, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239291 -> initscore=-1.156570\n",
      "[LightGBM] [Info] Start training from score -1.156570\n",
      "[LightGBM] [Info] Number of positive: 7479, number of negative: 23780\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 737\n",
      "[LightGBM] [Info] Number of data points in the train set: 31259, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239259 -> initscore=-1.156746\n",
      "[LightGBM] [Info] Start training from score -1.156746\n",
      "[LightGBM] [Info] Number of positive: 9349, number of negative: 29724\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 764\n",
      "[LightGBM] [Info] Number of data points in the train set: 39073, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.239270 -> initscore=-1.156685\n",
      "[LightGBM] [Info] Start training from score -1.156685\n"
     ]
    }
   ],
   "source": [
    "# here we train the selective classifiers\n",
    "plg.fit(df_train[atts], df_train[\"TARGET\"])\n",
    "plg_auc.fit(df_train[atts], df_train[\"TARGET\"])\n",
    "scr.fit(df_train_[atts], df_train_[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#calibrate selective classifiers for target coverages\n",
    "target_coverages = [.99, .75, .50] # here we define the target coverages for the selective classifiers\n",
    "plg.calibrate(df_cal[atts], target_coverages=target_coverages)\n",
    "plg_auc.calibrate(df_cal[atts], df_cal[\"TARGET\"], target_coverages=target_coverages)\n",
    "scr.calibrate(target_coverages=target_coverages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# // Evaluate selective classifiers\n",
    "# here we create an array containing levels for acceptance depending on how the selective classifiers are calibrated\n",
    "# since we used three target coverages, we have four levels of acceptance, i.e., 0, 1, 2, 3\n",
    "selected_plg = plg.qband(df_test[atts])\n",
    "selected_plg_auc = plg_auc.qband(df_test[atts])\n",
    "selected_scr = scr.qband(df_test[atts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLG - full coverage:  0.8756269833145665\n",
      "PLG - predicting on 99% of cases:  0.8792604833712043\n",
      "PLG - predicting on 75% of cases:  0.9482135688478522\n",
      "PLG - predicting on 50% of cases:  0.9895833333333334\n"
     ]
    }
   ],
   "source": [
    "# check that the selective classifiers are able to identify instances where they make less mistakes\n",
    "print(\"PLG - full coverage: \", accuracy_score(df_test[\"TARGET\"][selected_plg>=0], plg.predict(df_test[atts][selected_plg>=0])))\n",
    "print(\"PLG - predicting on 99% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg>=1], plg.predict(df_test[atts][selected_plg>=1])))\n",
    "print(\"PLG - predicting on 75% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg>=2], plg.predict(df_test[atts][selected_plg>=2])))\n",
    "print(\"PLG - predicting on 50% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg>=3], plg.predict(df_test[atts][selected_plg>=3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLG AUC - full coverage:  0.8756269833145665\n",
      "PLG AUC - predicting on 99% of cases:  0.876993166287016\n",
      "PLG AUC - predicting on 75% of cases:  0.916712479384277\n",
      "PLG AUC - predicting on 50% of cases:  0.9800558436378142\n"
     ]
    }
   ],
   "source": [
    "print(\"PLG AUC - full coverage: \", accuracy_score(df_test[\"TARGET\"][selected_plg_auc>=0], plg_auc.predict(df_test[atts][selected_plg_auc>=0])))\n",
    "print(\"PLG AUC - predicting on 99% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg_auc>=1], plg_auc.predict(df_test[atts][selected_plg_auc>=1])))\n",
    "print(\"PLG AUC - predicting on 75% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg_auc>=2], plg_auc.predict(df_test[atts][selected_plg_auc>=2])))\n",
    "print(\"PLG AUC - predicting on 50% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_plg_auc>=3], plg_auc.predict(df_test[atts][selected_plg_auc>=3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCR - full coverage:  0.8759340771829256\n",
      "SCR - predicting on 99% of cases:  0.8794721105268585\n",
      "SCR - predicting on 75% of cases:  0.9488873904248145\n",
      "SCR - predicting on 50% of cases:  0.9901368760064412\n"
     ]
    }
   ],
   "source": [
    "print(\"SCR - full coverage: \", accuracy_score(df_test[\"TARGET\"][selected_scr>=0], scr.predict(df_test[atts][selected_scr>=0])))\n",
    "print(\"SCR - predicting on 99% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_scr>=1], scr.predict(df_test[atts][selected_scr>=1])))\n",
    "print(\"SCR - predicting on 75% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_scr>=2], scr.predict(df_test[atts][selected_scr>=2])))\n",
    "print(\"SCR - predicting on 50% of cases: \", accuracy_score(df_test[\"TARGET\"][selected_scr>=3], scr.predict(df_test[atts][selected_scr>=3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# check what happens when changing metric to AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLG - full coverage:  0.9299814639133982\n",
      "PLG - predicting on 99% of cases:  0.9314342951664598\n",
      "PLG - predicting on 75% of cases:  0.9598424240705253\n",
      "PLG - predicting on 50% of cases:  0.9884235349349638\n"
     ]
    }
   ],
   "source": [
    "print(\"PLG - full coverage: \", roc_auc_score(df_test[\"TARGET\"][selected_plg>=0], plg.predict_proba(df_test[atts][selected_plg>=0])[:,1]))\n",
    "print(\"PLG - predicting on 99% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg>=1], plg.predict_proba(df_test[atts][selected_plg>=1])[:,1]))\n",
    "print(\"PLG - predicting on 75% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg>=2], plg.predict_proba(df_test[atts][selected_plg>=2])[:,1]))\n",
    "print(\"PLG - predicting on 50% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg>=3], plg.predict_proba(df_test[atts][selected_plg>=3])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLG AUC - full coverage:  0.9299814639133982\n",
      "PLG AUC - predicting on 99% of cases:  0.9319424790167712\n",
      "PLG AUC - predicting on 75% of cases:  0.9684671594950266\n",
      "PLG AUC - predicting on 50% of cases:  0.9919272689795878\n"
     ]
    }
   ],
   "source": [
    "print(\"PLG AUC - full coverage: \", roc_auc_score(df_test[\"TARGET\"][selected_plg_auc>=0], plg_auc.predict_proba(df_test[atts][selected_plg_auc>=0])[:,1]))\n",
    "print(\"PLG AUC - predicting on 99% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg_auc>=1], plg_auc.predict_proba(df_test[atts][selected_plg_auc>=1])[:,1]))\n",
    "print(\"PLG AUC - predicting on 75% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg_auc>=2], plg_auc.predict_proba(df_test[atts][selected_plg_auc>=2])[:,1]))\n",
    "print(\"PLG AUC - predicting on 50% of cases: \", roc_auc_score(df_test[\"TARGET\"][selected_plg_auc>=3], plg_auc.predict_proba(df_test[atts][selected_plg_auc>=3])[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
